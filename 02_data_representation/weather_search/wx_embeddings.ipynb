{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Weather Data\n",
    "\n",
    "An embedding is a low-dimensional, vector representation of a (typically) high-dimensional feature which maintains the semantic meaning of the feature in a such a way that similar features are close in the embedding space.\n",
    "\n",
    "In this notebook, we use autoencoders to create embeddings for HRRR images. We can then use the embeddings to search for \"similar\" weather patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get -y install libeccodes0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q cfgrib xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "print(beam.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading HRRR data and converting to TensorFlow Records\n",
    "\n",
    "HRRR data comes in a Grib2 files on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.*.wrfsfcf00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME=\"gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t18z.wrfsfcf06.grib2\"   # derecho in the Midwest\n",
    "!gsutil ls -l {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    ds = cfgrib.open_datasets(TMPFILE)\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose one of the following:\n",
    "```\n",
    "    filter_by_keys={'typeOfLevel': 'unknown'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'surface'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGround'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermal'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricInhPa'}\n",
    "    filter_by_keys={'typeOfLevel': 'pressureFromGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigmaLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'meanSea'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigma'}\n",
    "    filter_by_keys={'typeOfLevel': 'depthBelowLand'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudBase'}\n",
    "    filter_by_keys={'typeOfLevel': 'nominalTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermZero'}\n",
    "    filter_by_keys={'typeOfLevel': 'adiabaticCondensation'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "import numpy as np\n",
    "\n",
    "refc\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    #ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'stepType': 'instant'}})\n",
    "    #ds.data_vars['prate'].plot()  # crain, prate\n",
    "    #ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'unknown', 'stepType': 'instant'}})\n",
    "    ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})\n",
    "    refc = ds.data_vars['refc']\n",
    "    refc.plot()\n",
    "    print(np.array([refc.sizes['y'], refc.sizes['x']]))\n",
    "    print(refc.time.data)\n",
    "    print(refc.valid_time.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(str(refc.time.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _array_feature(value, min_value, max_value):\n",
    "    \"\"\"Wrapper for inserting ndarray float features into Example proto.\"\"\"\n",
    "    value = np.nan_to_num(value.flatten()) # nan, -inf, +inf to numbers\n",
    "    value = np.clip(value, min_value, max_value) # clip to valid\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tfrecord(filename):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "        tf.io.gfile.copy(filename, TMPFILE, overwrite=True)\n",
    "        ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})\n",
    "   \n",
    "        # create a TF Record with the raw data\n",
    "        tfexample = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'ref': _array_feature(ds.data_vars['refc'].data, min_value=0, max_value=60),\n",
    "        }))\n",
    "        return tfexample.SerializeToString()\n",
    "\n",
    "s = create_tfrecord(FILENAME)\n",
    "print(len(s), s[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def generate_filenames(startdate: str, enddate: str):\n",
    "    start_dt = datetime.strptime(startdate, '%Y%m%d')\n",
    "    end_dt = datetime.strptime(enddate, '%Y%m%d')\n",
    "    dt = start_dt\n",
    "    while dt <= end_dt:\n",
    "        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2\n",
    "        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(\n",
    "                'gs://high-resolution-rapid-refresh',\n",
    "                dt.year, dt.month, dt.day, dt.hour)\n",
    "        dt = dt + timedelta(hours=1)\n",
    "        yield f\n",
    "        \n",
    "def generate_shuffled_filenames(startdate: str, enddate: str):\n",
    "    \"\"\"\n",
    "    shuffle the files so that a batch of records doesn't contain highly correlated entries\n",
    "    \"\"\"\n",
    "    filenames = [f for f in generate_filenames(startdate, enddate)]\n",
    "    np.random.shuffle(filenames)\n",
    "    return filenames\n",
    "\n",
    "print(generate_shuffled_filenames('20190915', '20190917'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wxsearch/hrrr_to_tfrecord.py\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "import subprocess\n",
    "import apache_beam as beam\n",
    "import random\n",
    "\n",
    "def _array_feature(value, min_value, max_value):\n",
    "    if isinstance(value, type(tf.constant(0))): # if value is tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    " \n",
    "    \"\"\"Wrapper for inserting ndarray float features into Example proto.\"\"\"\n",
    "    value = np.nan_to_num(value.flatten()) # nan, -inf, +inf to numbers\n",
    "    value = np.clip(value, min_value, max_value) # clip to valid\n",
    "    logging.info('Range of image values {} to {}'.format(np.min(value), np.max(value)))\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _string_feature(value):\n",
    "    return _bytes_feature(value.encode('utf-8'))\n",
    "\n",
    "def create_tfrecord(filename):\n",
    "    print(filename)\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "        tf.io.gfile.copy(filename, TMPFILE, overwrite=True)\n",
    "        ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})\n",
    "   \n",
    "        # create a TF Record with the raw data\n",
    "        refc = ds.data_vars['refc']\n",
    "        size = np.array([ds.data_vars['refc'].sizes['y']*1.0, ds.data_vars['refc'].sizes['x']*1.0])\n",
    "        tfexample = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'size': tf.train.Feature(float_list=tf.train.FloatList(value=size)),\n",
    "                    'ref': _array_feature(refc.data, min_value=0, max_value=60),\n",
    "                    'time': _string_feature(str(refc.time.data)),\n",
    "                    'valid_time': _string_feature(str(refc.valid_time.data))\n",
    "        }))\n",
    "        return tfexample.SerializeToString()\n",
    "\n",
    "def generate_filenames(startdate: str, enddate: str):\n",
    "    start_dt = datetime.strptime(startdate, '%Y%m%d')\n",
    "    end_dt = datetime.strptime(enddate, '%Y%m%d')\n",
    "    logging.info('Hourly records from {} to {}'.format(start_dt, end_dt))\n",
    "    dt = start_dt\n",
    "    while dt < end_dt:\n",
    "        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2\n",
    "        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(\n",
    "                'gs://high-resolution-rapid-refresh',\n",
    "                dt.year, dt.month, dt.day, dt.hour)\n",
    "        dt = dt + timedelta(hours=1)\n",
    "        yield f\n",
    "                 \n",
    "def generate_shuffled_filenames(startdate: str, enddate: str):\n",
    "    \"\"\"\n",
    "    shuffle the files so that a batch of records doesn't contain highly correlated entries\n",
    "    \"\"\"\n",
    "    filenames = [f for f in generate_filenames(startdate, enddate)]\n",
    "    np.random.shuffle(filenames)\n",
    "    return filenames\n",
    "\n",
    "def run_job(options):\n",
    "    # start the pipeline\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    with beam.Pipeline(options['runner'], options=opts) as p:\n",
    "        # create examples\n",
    "        examples = (\n",
    "          p\n",
    "          | 'hrrr_files' >> beam.Create(\n",
    "              generate_shuffled_filenames(options['startdate'], options['enddate']))\n",
    "          | 'create_tfr' >>\n",
    "          beam.Map(lambda x: create_tfrecord(x))\n",
    "        )\n",
    "\n",
    "        # write out tfrecords\n",
    "        _ = (examples\n",
    "              | 'write_tfr' >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                  os.path.join(options['outdir'], 'tfrecord')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "      description='Create training/eval files for lightning prediction')\n",
    "    parser.add_argument(\n",
    "      '--project',\n",
    "      default='',\n",
    "      help='Specify GCP project to bill to run on cloud')\n",
    "    parser.add_argument(\n",
    "      '--outdir', required=True, help='output dir. could be local or on GCS')\n",
    "  \n",
    "    parser.add_argument(\n",
    "      '--startdate',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='eg 20200915')\n",
    "    parser.add_argument(\n",
    "      '--enddate',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='eg 20200916 -- this is exclusive')\n",
    "    \n",
    "    \n",
    "    # parse command-line args and add a few more\n",
    "    logging.basicConfig(level=getattr(logging, 'INFO', None))\n",
    "    options = parser.parse_args().__dict__\n",
    "    outdir = options['outdir']\n",
    "    options.update({\n",
    "      'staging_location':\n",
    "          os.path.join(outdir, 'tmp', 'staging'),\n",
    "      'temp_location':\n",
    "          os.path.join(outdir, 'tmp'),\n",
    "      'job_name':\n",
    "          'wxsearch-' + datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "      'teardown_policy':\n",
    "          'TEARDOWN_ALWAYS',\n",
    "      'max_num_workers':\n",
    "          20,\n",
    "      'machine_type':\n",
    "          'n1-standard-8',\n",
    "      'region':\n",
    "          'us-central1',\n",
    "      'setup_file':\n",
    "          os.path.join(os.path.dirname(os.path.abspath(__file__)), './setup.py'),\n",
    "      'save_main_session':\n",
    "          True,\n",
    "      # 'sdk_location':\n",
    "      #    './local/beam/sdks/python/dist/apache-beam-2.12.0.tar.gz'\n",
    "    })\n",
    "\n",
    "    if not options['project']:\n",
    "        print('Launching local job ... hang on')\n",
    "        shutil.rmtree(outdir, ignore_errors=True)\n",
    "        os.makedirs(outdir)\n",
    "        options['runner'] = 'DirectRunner'\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ... hang on'.format(options['job_name']))\n",
    "        try:\n",
    "            subprocess.check_call('gsutil -m rm -r {}'.format(outdir).split())\n",
    "        except:  # pylint: disable=bare-except\n",
    "            pass\n",
    "        options['runner'] = 'DataflowRunner'\n",
    "\n",
    "    run_job(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m wxsearch.hrrr_to_tfrecord -- --startdate 20190915 --enddate 20190916 --project ai-analytics-solutions --outdir gs://ai-analytics-solutions-kfpdemo/wxsearch\n",
    "# --outdir tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading what was written out\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_dataset(pattern):\n",
    "    filenames = tf.io.gfile.glob(pattern)\n",
    "    ds = tf.data.TFRecordDataset(filenames, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "    return ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds = read_dataset('tmp/tfrecord*')\n",
    "tfexample = next(iter(ds))\n",
    "parsed = tf.train.Example.FromString(tfexample.numpy())\n",
    "print(parsed.features.feature['size'])\n",
    "print(parsed.features.feature['valid_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m54"
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.25.0.dev0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
