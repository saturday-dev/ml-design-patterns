{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Weather Data\n",
    "\n",
    "An embedding is a low-dimensional, vector representation of a (typically) high-dimensional feature which maintains the semantic meaning of the feature in a such a way that similar features are close in the embedding space.\n",
    "\n",
    "In this notebook, we use autoencoders to create embeddings for HRRR images. We can then use the embeddings to search for \"similar\" weather patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -y install libeccodes0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q cfgrib xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading HRRR data and converting to TensorFlow Records\n",
    "\n",
    "HRRR data comes in a Grib2 files on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.*.wrfsfcf00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME=\"gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t18z.wrfsfcf06.grib2\"   # derecho in the Midwest\n",
    "!gsutil ls -l {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    ds = cfgrib.open_datasets(TMPFILE)\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose one of the following:\n",
    "```\n",
    "    filter_by_keys={'typeOfLevel': 'unknown'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'surface'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGround'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermal'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricInhPa'}\n",
    "    filter_by_keys={'typeOfLevel': 'pressureFromGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigmaLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'meanSea'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigma'}\n",
    "    filter_by_keys={'typeOfLevel': 'depthBelowLand'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudBase'}\n",
    "    filter_by_keys={'typeOfLevel': 'nominalTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermZero'}\n",
    "    filter_by_keys={'typeOfLevel': 'adiabaticCondensation'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "import numpy as np\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    #ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'stepType': 'instant'}})\n",
    "    #ds.data_vars['prate'].plot()  # crain, prate\n",
    "    ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'unknown', 'stepType': 'instant'}})\n",
    "    ds.data_vars['refc'].plot()\n",
    "    print(np.array([ds.data_vars['refc'].sizes['y'], ds.data_vars['refc'].sizes['x']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _array_feature(value, min_value, max_value):\n",
    "    \"\"\"Wrapper for inserting ndarray float features into Example proto.\"\"\"\n",
    "    value = np.nan_to_num(value.flatten()) # nan, -inf, +inf to numbers\n",
    "    value = np.clip(value, min_value, max_value) # clip to valid\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tfrecord(filename):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "        tf.io.gfile.copy(filename, TMPFILE, overwrite=True)\n",
    "        ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'unknown', 'stepType': 'instant'}})\n",
    "   \n",
    "        # create a TF Record with the raw data\n",
    "        tfexample = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'ref': _array_feature(ds.data_vars['refc'].data, min_value=0, max_value=60),\n",
    "        }))\n",
    "        return tfexample.SerializeToString()\n",
    "\n",
    "s = create_tfrecord(FILENAME)\n",
    "print(len(s), s[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def generate_filenames(startdate: str, enddate: str):\n",
    "    start_dt = datetime.strptime(startdate, '%Y%m%d')\n",
    "    end_dt = datetime.strptime(enddate, '%Y%m%d')\n",
    "    dt = start_dt\n",
    "    while dt <= end_dt:\n",
    "        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2\n",
    "        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(\n",
    "                'gs://high-resolution-rapid-refresh',\n",
    "                dt.year, dt.month, dt.day, dt.hour)\n",
    "        dt = dt + timedelta(hours=1)\n",
    "        yield f\n",
    "print([x for x in generate_filenames('20190915', '20190917')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir wxsearch\n",
    "!touch wxsearch/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wxsearch/hrrr_to_tfrecord.py\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "import subprocess\n",
    "import apache_beam as beam\n",
    "import random\n",
    "\n",
    "def _array_feature(value, min_value, max_value):\n",
    "    if isinstance(value, type(tf.constant(0))): # if value is tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    " \n",
    "    \"\"\"Wrapper for inserting ndarray float features into Example proto.\"\"\"\n",
    "    value = np.nan_to_num(value.flatten()) # nan, -inf, +inf to numbers\n",
    "    value = np.clip(value, min_value, max_value) # clip to valid\n",
    "    print(value[:10])\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tfrecord(filename):\n",
    "    print(filename)\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "        tf.io.gfile.copy(filename, TMPFILE, overwrite=True)\n",
    "        ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'unknown', 'stepType': 'instant'}})\n",
    "   \n",
    "        # create a TF Record with the raw data\n",
    "        refc = ds.data_vars['refc']\n",
    "        size = np.array([ds.data_vars['refc'].sizes['y']*1.0, ds.data_vars['refc'].sizes['x']*1.0])\n",
    "        tfexample = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'size': tf.train.Feature(float_list=tf.train.FloatList(value=size)),\n",
    "                    #'ref': _array_feature(refc.data, min_value=0, max_value=60),\n",
    "        }))\n",
    "        return tfexample.SerializeToString()\n",
    "\n",
    "def generate_filenames(startdate: str, enddate: str):\n",
    "    start_dt = datetime.strptime(startdate, '%Y%m%d')\n",
    "    end_dt = datetime.strptime(enddate, '%Y%m%d')\n",
    "    print('Hourly records from {} to {}'.format(start_dt, end_dt))\n",
    "    dt = start_dt\n",
    "    while dt < end_dt:\n",
    "        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2\n",
    "        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(\n",
    "                'gs://high-resolution-rapid-refresh',\n",
    "                dt.year, dt.month, dt.day, dt.hour)\n",
    "        dt = dt + timedelta(hours=1)\n",
    "        yield f\n",
    "\n",
    "def run_job(options):\n",
    "    # start the pipeline\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    with beam.Pipeline(options['runner'], options=opts) as p:\n",
    "        # create examples\n",
    "        examples = (\n",
    "          p\n",
    "          | 'hrrr_files' >> beam.Create(\n",
    "              generate_filenames(options['startdate'], options['enddate']))\n",
    "          | 'create_tfr' >>\n",
    "          beam.FlatMap(lambda x: create_tfrecord(x))\n",
    "        )\n",
    "\n",
    "        # shuffle the examples so that each small batch doesn't contain\n",
    "        # highly correlated records\n",
    "        #examples = (examples\n",
    "        #      | 'reshuffleA' >> beam.Map(\n",
    "        #          lambda t: (random.randint(1, 1000), t))\n",
    "        #      | 'reshuffleB' >> beam.GroupByKey()\n",
    "        #      | 'reshuffleC' >> beam.FlatMap(lambda t: t[1]))\n",
    "\n",
    "        # write out tfrecords\n",
    "        _ = (examples\n",
    "              | 'write_tfr' >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                  os.path.join(options['outdir'], 'tfrecord')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "      description='Create training/eval files for lightning prediction')\n",
    "    parser.add_argument(\n",
    "      '--project',\n",
    "      default='',\n",
    "      help='Specify GCP project to bill to run on cloud')\n",
    "    parser.add_argument(\n",
    "      '--outdir', required=True, help='output dir. could be local or on GCS')\n",
    "  \n",
    "    parser.add_argument(\n",
    "      '--startdate',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='eg 20200915')\n",
    "    parser.add_argument(\n",
    "      '--enddate',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='eg 20200916 -- this is exclusive')\n",
    "    \n",
    "    \n",
    "    # parse command-line args and add a few more\n",
    "    logging.basicConfig(level=getattr(logging, 'INFO', None))\n",
    "    options = parser.parse_args().__dict__\n",
    "    outdir = options['outdir']\n",
    "    options.update({\n",
    "      'staging_location':\n",
    "          os.path.join(outdir, 'tmp', 'staging'),\n",
    "      'temp_location':\n",
    "          os.path.join(outdir, 'tmp'),\n",
    "      'job_name':\n",
    "          'wxsearch-' + datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "      'teardown_policy':\n",
    "          'TEARDOWN_ALWAYS',\n",
    "      'max_num_workers':\n",
    "          20,\n",
    "      'machine_type':\n",
    "          'n1-standard-8',\n",
    "      'region':\n",
    "          'us-central1',\n",
    "      'setup_file':\n",
    "          os.path.join(os.path.dirname(os.path.abspath(__file__)), './setup.py'),\n",
    "      'save_main_session':\n",
    "          True,\n",
    "      # 'sdk_location':\n",
    "      #    './local/beam/sdks/python/dist/apache-beam-2.12.0.tar.gz'\n",
    "    })\n",
    "\n",
    "    if not options['project']:\n",
    "        print('Launching local job ... hang on')\n",
    "        shutil.rmtree(outdir, ignore_errors=True)\n",
    "        os.makedirs(outdir)\n",
    "        options['runner'] = 'DirectRunner'\n",
    "    else:\n",
    "        print('Launching Dataflow job {} ... hang on'.format(options['job_name']))\n",
    "        try:\n",
    "            subprocess.check_call('gsutil -m rm -r {}'.format(outdir).split())\n",
    "        except:  # pylint: disable=bare-except\n",
    "            pass\n",
    "        options['runner'] = 'DataflowRunner'\n",
    "\n",
    "    run_job(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KMP_DUPLICATE_LIB_OK=TRUE\n",
    "python3 -m wxsearch.hrrr_to_tfrecord \\\n",
    "    --startdate 20190915 --enddate 20190916 --outdir tmp # --project ai-analytics-solutions --outdir gs://ai-analytics-solutions-kfpdemo/wxsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
