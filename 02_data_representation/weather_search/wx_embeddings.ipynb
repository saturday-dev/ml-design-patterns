{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Weather Data\n",
    "\n",
    "An embedding is a low-dimensional, vector representation of a (typically) high-dimensional feature which maintains the semantic meaning of the feature in a such a way that similar features are close in the embedding space.\n",
    "\n",
    "In this notebook, we use autoencoders to create embeddings for HRRR images. We can then use the embeddings to search for \"similar\" weather patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -y --quiet install libeccodes0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q cfgrib xarray pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "print(beam.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading HRRR data and converting to TensorFlow Records\n",
    "\n",
    "HRRR data comes in a Grib2 files on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.*.wrfsfcf00*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME=\"gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t18z.wrfsfcf06.grib2\"   # derecho in the Midwest\n",
    "!gsutil ls -l {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    ds = cfgrib.open_datasets(TMPFILE)\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose one of the following:\n",
    "```\n",
    "    filter_by_keys={'typeOfLevel': 'unknown'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'surface'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGround'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermal'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricInhPa'}\n",
    "    filter_by_keys={'typeOfLevel': 'pressureFromGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigmaLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'meanSea'}\n",
    "    filter_by_keys={'typeOfLevel': 'heightAboveGroundLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'sigma'}\n",
    "    filter_by_keys={'typeOfLevel': 'depthBelowLand'}\n",
    "    filter_by_keys={'typeOfLevel': 'isobaricLayer'}\n",
    "    filter_by_keys={'typeOfLevel': 'cloudBase'}\n",
    "    filter_by_keys={'typeOfLevel': 'nominalTop'}\n",
    "    filter_by_keys={'typeOfLevel': 'isothermZero'}\n",
    "    filter_by_keys={'typeOfLevel': 'adiabaticCondensation'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import cfgrib\n",
    "import numpy as np\n",
    "\n",
    "refc = 0\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "    tf.io.gfile.copy(FILENAME, TMPFILE, overwrite=True)\n",
    "    #ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'stepType': 'instant'}})\n",
    "    #ds.data_vars['prate'].plot()  # crain, prate\n",
    "    ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'unknown', 'stepType': 'instant'}})\n",
    "    #ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})\n",
    "    refc = ds.data_vars['refc']\n",
    "    refc.plot()\n",
    "    print(np.array([refc.sizes['y'], refc.sizes['x']]))\n",
    "    print(refc.time.data)\n",
    "    print(refc.valid_time.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(refc.time.data)[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _array_feature(value, min_value, max_value):\n",
    "    \"\"\"Wrapper for inserting ndarray float features into Example proto.\"\"\"\n",
    "    value = np.nan_to_num(value.flatten()) # nan, -inf, +inf to numbers\n",
    "    value = np.clip(value, min_value, max_value) # clip to valid\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_tfrecord(filename):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        TMPFILE=\"{}/read_grib\".format(tmpdirname)\n",
    "        tf.io.gfile.copy(filename, TMPFILE, overwrite=True)\n",
    "        ds = xr.open_dataset(TMPFILE, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}})\n",
    "   \n",
    "        # create a TF Record with the raw data\n",
    "        tfexample = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'ref': _array_feature(ds.data_vars['refc'].data, min_value=0, max_value=60),\n",
    "        }))\n",
    "        return tfexample.SerializeToString()\n",
    "\n",
    "s = create_tfrecord(FILENAME)\n",
    "print(len(s), s[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def generate_filenames(startdate: str, enddate: str):\n",
    "    start_dt = datetime.strptime(startdate, '%Y%m%d')\n",
    "    end_dt = datetime.strptime(enddate, '%Y%m%d')\n",
    "    dt = start_dt\n",
    "    while dt <= end_dt:\n",
    "        # gs://high-resolution-rapid-refresh/hrrr.20200811/conus/hrrr.t04z.wrfsfcf00.grib2\n",
    "        f = '{}/hrrr.{:4}{:02}{:02}/conus/hrrr.t{:02}z.wrfsfcf00.grib2'.format(\n",
    "                'gs://high-resolution-rapid-refresh',\n",
    "                dt.year, dt.month, dt.day, dt.hour)\n",
    "        dt = dt + timedelta(hours=1)\n",
    "        yield f\n",
    "        \n",
    "def generate_shuffled_filenames(startdate: str, enddate: str):\n",
    "    \"\"\"\n",
    "    shuffle the files so that a batch of records doesn't contain highly correlated entries\n",
    "    \"\"\"\n",
    "    filenames = [f for f in generate_filenames(startdate, enddate)]\n",
    "    np.random.shuffle(filenames)\n",
    "    return filenames\n",
    "\n",
    "print(generate_shuffled_filenames('20190915', '20190917'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m wxsearch.hrrr_to_tfrecord -- --startdate 20190915 --enddate 20190916  --outdir gs://ai-analytics-solutions-kfpdemo/wxsearch/data --project ai-analytics-solutions\n",
    "# --outdir tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://ai-analytics-solutions-kfpdemo/wxsearch/data/tfrecord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the written TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading what was written out\n",
    "import tensorflow as tf\n",
    "\n",
    "def parse_tfrecord(example_data):\n",
    "    parsed = tf.io.parse_single_example(example_data, {\n",
    "        'size': tf.io.VarLenFeature(tf.float32),\n",
    "        'ref': tf.io.VarLenFeature(tf.float32),\n",
    "        'time': tf.io.FixedLenFeature([], tf.string),\n",
    "        'valid_time': tf.io.FixedLenFeature([], tf.string)\n",
    "     })\n",
    "    parsed['size'] = tf.sparse.to_dense(parsed['size'])\n",
    "    parsed['ref'] = tf.reshape(tf.sparse.to_dense(parsed['ref']), (1059, 1799))/60. # 0 to 1\n",
    "    return parsed\n",
    "\n",
    "def read_dataset(pattern):\n",
    "    filenames = tf.io.gfile.glob(pattern)\n",
    "    ds = tf.data.TFRecordDataset(filenames, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "    return ds.prefetch(tf.data.experimental.AUTOTUNE).map(parse_tfrecord)\n",
    "\n",
    "ds = read_dataset('gs://ai-analytics-solutions-kfpdemo/wxsearch/data/tfrecord-00000-*')\n",
    "for refc in ds.take(1):\n",
    "    print(repr(refc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create autoencoder in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = tf.keras.Input(shape=(1059, 1799, 1), name='refc_input')\n",
    "\n",
    "x = tf.keras.layers.Cropping2D(cropping=((17, 18),(4, 3)), name='cropped')(input_img)\n",
    "nlayers = 3\n",
    "for layerno in range(nlayers):\n",
    "    x = tf.keras.layers.Conv2D(2**(nlayers-layerno + 3), 5, activation='relu', padding='same', name='encoder_conv_{}'.format(layerno))(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(4, padding='same', name='encoder_pool_{}'.format(layerno))(x)\n",
    "x = tf.keras.layers.Lambda(lambda x: x, name='refc_embedding')(x)\n",
    "for layerno in range(nlayers):\n",
    "    x = tf.keras.layers.Conv2D(2**(layerno + 4), 5, activation='relu', padding='same', name='decoder_conv_{}'.format(layerno))(x)\n",
    "    x = tf.keras.layers.UpSampling2D(4, name='decoder_upsamp_{}'.format(layerno))(x)\n",
    "x = tf.keras.layers.Conv2D(1, 3, activation='sigmoid', padding='same', name='before_padding')(x)\n",
    "decoded = tf.keras.layers.ZeroPadding2D(padding=((17,18),(4,3)), name='refc_reconstructed')(x)\n",
    "\n",
    "autoencoder = tf.keras.Model(input_img, decoded, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(autoencoder, to_file='autoencoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_and_label(rec):\n",
    "    return rec['ref'], rec['ref']\n",
    "\n",
    "ds = read_dataset('gs://ai-analytics-solutions-kfpdemo/wxsearch/data/tfrecord-*').map(input_and_label).batch(2).repeat()\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('tmp/checkpoints')\n",
    "history = autoencoder.fit(ds, steps_per_epoch=1, epochs=3, shuffle=True, callbacks=[checkpoint])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('tmp/savedmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wxsearch/train_autoencoder.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_tfrecord(example_data):\n",
    "    parsed = tf.io.parse_single_example(example_data, {\n",
    "        'size': tf.io.VarLenFeature(tf.float32),\n",
    "        'ref': tf.io.VarLenFeature(tf.float32),\n",
    "        'time': tf.io.FixedLenFeature([], tf.string),\n",
    "        'valid_time': tf.io.FixedLenFeature([], tf.string)\n",
    "     })\n",
    "    parsed['size'] = tf.sparse.to_dense(parsed['size'])\n",
    "    parsed['ref'] = tf.reshape(tf.sparse.to_dense(parsed['ref']), (1059, 1799))/60. # 0 to 1\n",
    "    return parsed\n",
    "\n",
    "def read_dataset(pattern):\n",
    "    filenames = tf.io.gfile.glob(pattern)\n",
    "    ds = tf.data.TFRecordDataset(filenames, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "    return ds.prefetch(tf.data.experimental.AUTOTUNE).map(parse_tfrecord)\n",
    "\n",
    "def create_model(nlayers=3, poolsize=4):\n",
    "    input_img = tf.keras.Input(shape=(1059, 1799, 1), name='refc_input')\n",
    "\n",
    "    x = tf.keras.layers.Cropping2D(cropping=((17, 18),(4, 3)), name='cropped')(input_img)\n",
    "    for layerno in range(nlayers):\n",
    "        x = tf.keras.layers.Conv2D(2**(nlayers-layerno + 3), 5, activation='relu', padding='same', name='encoder_conv_{}'.format(layerno))(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(poolsize, padding='same', name='encoder_pool_{}'.format(layerno))(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: x, name='refc_embedding')(x)\n",
    "    for layerno in range(nlayers):\n",
    "        x = tf.keras.layers.Conv2D(2**(layerno + 4), 5, activation='relu', padding='same', name='decoder_conv_{}'.format(layerno))(x)\n",
    "        x = tf.keras.layers.UpSampling2D(poolsize, name='decoder_upsamp_{}'.format(layerno))(x)\n",
    "    x = tf.keras.layers.Conv2D(1, 3, activation='sigmoid', padding='same', name='before_padding')(x)\n",
    "    decoded = tf.keras.layers.ZeroPadding2D(padding=((17,18),(4,3)), name='refc_reconstructed')(x)\n",
    "\n",
    "    autoencoder = tf.keras.Model(input_img, decoded, name='autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "def run_job(opts):\n",
    "    def input_and_label(rec):\n",
    "        return rec['ref'], rec['ref']\n",
    "    ds = read_dataset(opts['input']).map(input_and_label).batch(opts['batch_size']).repeat()\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(opts['outdir'], 'checkpoints'))\n",
    "    \n",
    "    autoencoder = create_model()\n",
    "    history = autoencoder.fit(ds, steps_per_epoch=opts['num_steps']//opts['num_checkpoints'],\n",
    "                              epochs=opts['num_checkpoints'], shuffle=True, callbacks=[checkpoint])\n",
    "    \n",
    "    autoencoder.save(os.path.join(opts['outdir'], 'savedmodel'))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "      description='Train an autoencoder')\n",
    "    parser.add_argument(\n",
    "      '--project',\n",
    "      default='',\n",
    "      help='Specify GCP project to bill to run on cloud')\n",
    "    parser.add_argument(\n",
    "      '--outdir', required=True, help='output dir. could be local or on GCS')\n",
    "    parser.add_argument(\n",
    "      '--input', required=True, help='input pattern. eg: gs://ai-analytics-solutions-kfpdemo/wxsearch/data/tfrecord-*')\n",
    "    parser.add_argument(\n",
    "      '--batch_size', default=2, help='batch size for training')\n",
    "    parser.add_argument(\n",
    "      '--num_steps', default=12, help='total number of steps for training')\n",
    "    parser.add_argument(\n",
    "      '--num_checkpoints', default=3, help='number of steps for training')\n",
    "     \n",
    "     \n",
    "    # parse command-line args and add a few more\n",
    "    logging.basicConfig(level=getattr(logging, 'INFO', None))\n",
    "    options = parser.parse_args().__dict__\n",
    "\n",
    "    if not options['project']:\n",
    "        print('Removing local output directory ... hang on')\n",
    "        shutil.rmtree(outdir, ignore_errors=True)\n",
    "        os.makedirs(outdir)\n",
    "    else:\n",
    "        print('Removing GCS output directory ... hang on')\n",
    "        try:\n",
    "            subprocess.check_call('gsutil -m rm -r {}'.format(outdir).split())\n",
    "        except:  # pylint: disable=bare-except\n",
    "            pass\n",
    "\n",
    "    run_job(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%run -m wxsearch.train_autoencoder -- --input gs://ai-analytics-solutions-kfpdemo/wxsearch/data/tfrecord-*  --outdir gs://ai-analytics-solutions-kfpdemo/wxsearch/trained --project ai-analytics-solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
